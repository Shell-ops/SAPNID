{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77f7259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 06:51:30.840011: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers.models.roberta.tokenization_roberta import RobertaTokenizer\n",
    "from transformers.optimization import AdamW\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779e9c77",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a2abf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "Cbase_pretrain_path = '../EMNLP2023/Dataset/Processed/HWU/Pretrain.txt'\n",
    "Cbase_x = []\n",
    "Cbase_y = []\n",
    "for line in open(Cbase_pretrain_path):\n",
    "    if len(line.split('\\t')) != 2:\n",
    "        continue\n",
    "    else:\n",
    "        Cbase_y.append(line.split('\\t')[1][:-1].replace('_',' '))\n",
    "        Cbase_x.append(line.split('\\t')[0])\n",
    "\n",
    "label_yd_yi_dic = {'play podcasts': '0', 'alarm set': '1', 'takeaway query': '2', 'social post': '3', 'email querycontact': '4', 'iot wemo off': '5', 'iot hue lightup': '6', 'general affirm': '7', 'calendar set': '8', 'general negate': '9', 'iot coffee': '10', 'news query': '11', 'recommendation events': '12', 'play audiobook': '13', 'play radio': '14', 'alarm remove': '15', 'iot hue lightdim': '16', 'datetime query': '17', 'recommendation movies': '18', 'social query': '19', 'play game': '20', 'datetime convert': '21', 'iot hue lightoff': '22', 'music likeness': '23', 'email sendemail': '24', 'general quirky': '25', 'weather query': '26', 'transport taxi': '27', 'lists query': '28', 'qa factoid': '29', 'transport traffic': '30', 'general joke': '31'}\n",
    "\n",
    "print(len(set(Cbase_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c76b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 model initialization\n",
    "# encoder\n",
    "max_length = 32\n",
    "n_tokens = 5\n",
    "hidden_size = 1024\n",
    "\n",
    "# 2 hyper-parameters initialization\n",
    "batch_size = 10\n",
    "cluster_nums = len(list(set(Cbase_y)))\n",
    "cluster_nums_str = str(cluster_nums)\n",
    "\n",
    "# 3 training parameters\n",
    "base_lr = 5e-6\n",
    "lr_scale = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae573c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n",
      "205\n",
      "205\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "Cn_path = '../EMNLP2023/Dataset/Processed/HWU/Aux1PMulti.txt'\n",
    "Cn = []\n",
    "for line in open(Cn_path):\n",
    "    Cn.append(line)\n",
    "random.shuffle(Cn)\n",
    "\n",
    "Cn_x = []\n",
    "Cn_y = []\n",
    "for line in Cn:\n",
    "    Cn_y.append(line.split('\\t')[1][:-1].replace('_',' '))\n",
    "    Cn_x.append(line.split('\\t')[0])\n",
    "print(len(Cn_x))\n",
    "print(len(Cn_y))\n",
    "\n",
    "pre_yd_list = label_yd_yi_dic.keys()\n",
    "pre_yd_yi_dic = label_yd_yi_dic\n",
    "\n",
    "Cn_y_id = []\n",
    "for i in range(len(Cn_y)):\n",
    "    if Cn_y[i] in pre_yd_list:\n",
    "        Cn_y_id.append(pre_yd_yi_dic[Cn_y[i]])\n",
    "    else:\n",
    "        Cn_y_id.append(cluster_nums_str)\n",
    "print(len(Cn_y_id))\n",
    "print(len(set(Cn_y_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5454576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### \n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as util_data\n",
    "\n",
    "class AugmentPairSamples(Dataset):\n",
    "    def __init__(self, train_x, train_y):\n",
    "        assert len(train_y) == len(train_x)\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'text': self.train_x[idx], 'label': self.train_y[idx]}\n",
    "\n",
    "train_dataset = AugmentPairSamples(Cn_x[:200], Cn_y_id[:200])\n",
    "train_loader = util_data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54a64f5",
   "metadata": {},
   "source": [
    "### 加载 Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3634e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0,1,2,3\"\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d00ead86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PREModel(nn.Module):\n",
    "    def __init__(self, pre_tokenizer, pre_model, device=device, training_flag=True):\n",
    "        \n",
    "        super(PREModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.classifier_loss1 = nn.BCELoss()\n",
    "        self.classifier_loss2 = nn.CrossEntropyLoss()\n",
    "        self.training_flag = training_flag\n",
    "        self.optimizer = None\n",
    "        \n",
    "        ###### SentenceBert Model ###### \n",
    "        self.tokenizer = pre_tokenizer\n",
    "        self.sentbert = pre_model.roberta_single\n",
    "        \n",
    "        ####### Classifer Model ######  \n",
    "        self.classifer = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 1)).to(self.device)\n",
    "        \n",
    "    ###### SentenceBert Model ###### \n",
    "    def get_embeddings(self, features, pooling=\"mean\"):\n",
    "        bert_output =  self.sentbert.forward(**features)\n",
    "        attention_mask = features['attention_mask'].unsqueeze(-1).to(device)\n",
    "        all_output = bert_output[0]\n",
    "        mean_output =  torch.sum(all_output*attention_mask, dim=1) / torch.sum(attention_mask, dim=1)\n",
    "        return mean_output\n",
    "\n",
    "    def set_optmizer(self, opt):\n",
    "        self.optimizer = opt\n",
    "        \n",
    "    def forward(self, inputs, labels, aggregate=True):\n",
    "        for s_idx in range(len(labels)): #10\n",
    "            h0 = self.get_embeddings(inputs[s_idx], pooling=\"mean\").to(self.device) #[20,1024]\n",
    "            y_vector = torch.tensor([0.]*h0.shape[0]).to(self.device) #[20]\n",
    "            y_idx = int(labels[s_idx])\n",
    "            y_onehot = y_vector.clone()\n",
    "            y_onehot[y_idx] = 1.\n",
    "            \n",
    "            if s_idx == 0:\n",
    "                classifier_out = self.classifer(h0).squeeze().unsqueeze(0) #[1,20]\n",
    "                y_onehot_all = y_onehot.unsqueeze(0)\n",
    "            else:\n",
    "                classifier_out = torch.cat([classifier_out,self.classifer(h0).squeeze().unsqueeze(0)],dim=0) #[10,20]\n",
    "                y_onehot_all = torch.cat([y_onehot_all,y_onehot.unsqueeze(0)],dim=0)\n",
    "\n",
    "        classifier_output = classifier_out\n",
    "        classifier_sigmode_vector = F.sigmoid(classifier_output)\n",
    "        classifier_softmax_vector = F.softmax(classifier_output,dim=1)\n",
    "        cluster_result = [str(torch.argmax(classifier_softmax_vector[i]).item()) for i in range(classifier_softmax_vector.shape[0])]\n",
    "            \n",
    "        class_loss_all = torch.tensor(0.0).to(device)\n",
    "        ### 获得 Loss\n",
    "        class_loss_s = 0.\n",
    "        class_loss_s1 = self.classifier_loss1(classifier_sigmode_vector,y_onehot_all)\n",
    "        class_loss_s2 = self.classifier_loss2(classifier_softmax_vector,y_onehot_all)\n",
    "        class_loss_all += class_loss_s1 + class_loss_s2\n",
    "        class_loss = torch.div(class_loss_all,h0.shape[0])\n",
    "        \n",
    "        print(\"cluster_output:\",cluster_result)\n",
    "        print(\"labels:\",labels)\n",
    "        print(\"ACC_train:\",accuracy_score(labels,cluster_result))\n",
    "        \n",
    "        if self.training_flag:\n",
    "            loss = class_loss\n",
    "            ##  反向传播\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return class_loss.detach(), class_loss_s1.detach(), class_loss_s2.detach(), labels, cluster_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17cb714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = '../EMNLP2023/Result/PreModel_HWU.pt'\n",
    "pre_model = torch.load(ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0dcb89",
   "metadata": {},
   "source": [
    "### 搭建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4ad67c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRAINModel(nn.Module):\n",
    "    def __init__(self, pre_tokenizer, pre_encoder, pre_classifer, device=device, training_flag=True):\n",
    "        \n",
    "        super(TRAINModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.classifier_loss1 = nn.BCELoss()\n",
    "        self.classifier_loss2 = nn.CrossEntropyLoss()\n",
    "        self.training_flag = training_flag\n",
    "        self.optimizer = None\n",
    "        \n",
    "        ###### SentenceBert Model ###### \n",
    "        self.tokenizer = pre_tokenizer\n",
    "        self.sentbert = pre_encoder\n",
    "        \n",
    "        ####### Classifer Model ######  \n",
    "        self.classifer = pre_classifer\n",
    "        \n",
    "    ###### SentenceBert Model ###### \n",
    "    def get_embeddings(self, features, pooling=\"mean\"):\n",
    "        bert_output =  self.sentbert.forward(**features)\n",
    "        attention_mask = features['attention_mask'].unsqueeze(-1).to(device)\n",
    "        all_output = bert_output[0]\n",
    "        mean_output =  torch.sum(all_output*attention_mask, dim=1) / torch.sum(attention_mask, dim=1)\n",
    "        return mean_output\n",
    "    \n",
    "    def get_none_embeddings(self, input_t, input_none, pooling=\"mean\"):\n",
    "        x_embed = self.sentbert.embeddings.word_embeddings(input_t['input_ids']) #[10,19,1024]\n",
    "        x_embed_cls = self.sentbert.embeddings.word_embeddings(input_t['input_ids'])[:,0,:].unsqueeze(1) #[10,1,1024]\n",
    "        none_embeddings_inital = self.sentbert.forward(**input_none).last_hidden_state \n",
    "        soft_text_prompt = nn.Embedding(n_tokens, hidden_size)\n",
    "        soft_text_prompt.weight = nn.parameter.Parameter(none_embeddings_inital) #[1,5,1024]\n",
    "        return x_embed_cls, x_embed, soft_text_prompt.weight\n",
    "\n",
    "    def set_optmizer(self, opt):\n",
    "        self.optimizer = opt\n",
    "        \n",
    "    def forward(self, inputs, inputs_text, inputs_none, labels, aggregate=True):\n",
    "        for s_idx in range(len(labels)): #10\n",
    "            h0 = self.get_embeddings(inputs[s_idx], pooling=\"mean\").to(self.device) #[20,1024]\n",
    "            \n",
    "            y_vector = torch.tensor([0.]*(h0.shape[0]+1)).to(self.device) #[20]\n",
    "            y_idx = int(labels[s_idx])\n",
    "            y_onehot = y_vector.clone()\n",
    "            y_onehot[y_idx] = 1.\n",
    "            \n",
    "            if s_idx == 0:\n",
    "                classifier_out = self.classifer(h0).squeeze().unsqueeze(0) #[1,20]\n",
    "                y_onehot_all = y_onehot.unsqueeze(0)\n",
    "            else:\n",
    "                classifier_out = torch.cat([classifier_out,self.classifer(h0).squeeze().unsqueeze(0)],dim=0) #[10,20]\n",
    "                y_onehot_all = torch.cat([y_onehot_all,y_onehot.unsqueeze(0)],dim=0)\n",
    "        print('y_onehot_all:',y_onehot_all.shape)\n",
    "        print('classifier_out:',classifier_out.shape)\n",
    "                \n",
    "        x_embed_cls, x_embed, soft_text_prompt_weight = self.get_none_embeddings(inputs_text.to(self.device),inputs_none.to(self.device))\n",
    "        x_attmask = inputs_text['attention_mask'] #[10, 19]\n",
    "        x_ones = torch.ones([batch_size,n_tokens]).to(device) #[10, 5]\n",
    "        none_attmask = torch.cat([x_ones,x_attmask],dim=1).to(device)\n",
    "\n",
    "        none_embedding = soft_text_prompt_weight.repeat(batch_size,1,1) + x_embed_cls.repeat(1,n_tokens,1)\n",
    "        none_encoderinput = torch.cat([none_embedding,x_embed],dim=1)\n",
    "\n",
    "        text_input = {\n",
    "            'inputs_embeds': none_encoderinput,\n",
    "            'attention_mask': none_attmask.to(device)\n",
    "        }\n",
    "        \n",
    "        h0_none = self.sentbert.forward(**text_input).pooler_output #[10,24,1024]\n",
    "        print('h0_none:',h0_none.shape)\n",
    "        classifier_out_none = self.classifer(h0_none)\n",
    "        classifier_output = torch.cat([classifier_out,classifier_out_none],dim=1)\n",
    "        print('classifier_output:',classifier_output.shape)\n",
    "        classifier_sigmode_vector = F.sigmoid(classifier_output)\n",
    "        classifier_softmax_vector = F.softmax(classifier_output,dim=1)\n",
    "        cluster_result = [str(torch.argmax(classifier_softmax_vector[i]).item()) for i in range(classifier_softmax_vector.shape[0])]\n",
    "        class_loss_all = torch.tensor(0.0).to(device)\n",
    "        ### 获得 Loss\n",
    "        class_loss_s = 0.\n",
    "        class_loss_s1 = self.classifier_loss1(classifier_sigmode_vector,y_onehot_all)\n",
    "        class_loss_s2 = self.classifier_loss2(classifier_softmax_vector,y_onehot_all)\n",
    "        class_loss_all += class_loss_s1 + class_loss_s2\n",
    "        class_loss = torch.div(class_loss_all,h0.shape[0])\n",
    "        \n",
    "        ACC_train = accuracy_score(labels,cluster_result)\n",
    "        F1_train = f1_score(labels, cluster_result, average='macro')\n",
    "        print(\"cluster_output:\",cluster_result)\n",
    "        print(\"labels:\",labels)\n",
    "        print(\"ACC_train:\",accuracy_score(labels,cluster_result))\n",
    "          \n",
    "        if self.training_flag:\n",
    "            loss = class_loss\n",
    "            ##  反向传播\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return class_loss.detach(), labels, cluster_result, ACC_train, F1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fed986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TRAINModel(pre_tokenizer=pre_model.tokenizer, pre_encoder=pre_model.sentbert, pre_classifer=pre_model.classifer, device=device, training_flag=True).to(device)\n",
    "base_lr = 5e-6\n",
    "lr_scale = 100\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':model.sentbert.parameters()}, \n",
    "    {'params':model.classifer.parameters(), 'lr':base_lr * lr_scale}], base_lr)\n",
    "\n",
    "model.set_optmizer(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3cf76e",
   "metadata": {},
   "source": [
    "### 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25122d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_task_input(batch, is_contrastive=False):\n",
    "    if is_contrastive:\n",
    "        text, label = batch['text'], batch['label']\n",
    "        all_label_descrip = list(label_yd_yi_dic.keys())\n",
    "        feat = {'input_ids': torch.tensor(0),'attention_mask':torch.tensor(0)}\n",
    "        features1 = model.tokenizer.batch_encode_plus(text, max_length=max_length, return_tensors='pt', padding='longest', \n",
    "                                                         truncation=True)\n",
    "        features2 = model.tokenizer.batch_encode_plus(all_label_descrip, max_length=max_length, return_tensors='pt', padding='longest', \n",
    "                                                         truncation=True)\n",
    "        features_none = model.tokenizer.batch_encode_plus(['none of them'], max_length=max_length, return_tensors='pt', padding='longest', \n",
    "                                                         truncation=True)\n",
    "        \n",
    "        feat1_inputids = features1[\"input_ids\"]\n",
    "        feat1_attmask = features1[\"attention_mask\"]\n",
    "        feat2_inputids = features2[\"input_ids\"]\n",
    "        feat2_attmask = features2[\"attention_mask\"]\n",
    "        max_len = feat1_inputids.shape[1] + feat2_inputids.shape[1] \n",
    "        \n",
    "        # count\n",
    "        non_zero1 = torch.count_nonzero(feat1_attmask, dim=1).reshape(-1, 1)\n",
    "        non_zero2 = torch.count_nonzero(feat2_attmask, dim=1).reshape(-1, 1)\n",
    "        \n",
    "        feats = []\n",
    "        for i in range(non_zero1.shape[0]):\n",
    "            index1 = non_zero1[i][0]\n",
    "            feat_s = {'input_ids': torch.tensor(0),'attention_mask':torch.tensor(0)}\n",
    "            for j in range(non_zero2.shape[0]):\n",
    "                index2 = non_zero2[j][0]\n",
    "                feat_inputid = torch.cat([feat2_inputids[j][:index2],feat1_inputids[i][1:index1]])\n",
    "                feat_attmask = torch.tensor([1]*(index1+index2)+[0]*(max_len-index1-index2)).unsqueeze(0)\n",
    "                feat_inputid = torch.cat([feat_inputid,torch.tensor([1]*(max_len-index1-index2+1))]).unsqueeze(0)\n",
    "                if j == 0:\n",
    "                    feat_inputid_s = feat_inputid\n",
    "                    feat_attmask_s = feat_attmask\n",
    "                elif j != 0:\n",
    "                    feat_inputid_s = torch.cat([feat_inputid_s, feat_inputid],dim=0)\n",
    "                    feat_attmask_s = torch.cat([feat_attmask_s, feat_attmask],dim=0)\n",
    "            feat_s['input_ids'] = feat_inputid_s.to(device)\n",
    "            feat_s['attention_mask'] = feat_attmask_s.to(device)\n",
    "            feats.append(feat_s)  \n",
    "        \n",
    "        return feats, features1, features_none, label\n",
    "    \n",
    "ClassLoss_all = []\n",
    "ACC_result = []\n",
    "F1_result = []\n",
    "def training(train_loader):\n",
    "    pre_acc = 0.\n",
    "    epoch = 0\n",
    "    for i in np.arange(2500):  ## 25 epochs\n",
    "        model.train()\n",
    "        model.training_flag = True        \n",
    "        print('************'+str(i)+'************')        \n",
    "        try:\n",
    "            batch = next(train_loader_iter)\n",
    "        except:\n",
    "            train_loader_iter = iter(train_loader)\n",
    "            batch = next(train_loader_iter)      \n",
    "        \n",
    "        feats, features1, features_none, label = prepare_task_input(batch, is_contrastive=True)    \n",
    "        losses = model.forward(feats, features1, features_none, label, aggregate=True)\n",
    "        \n",
    "        ACC_result.append(losses[3])\n",
    "        F1_result.append(losses[4])\n",
    "        print(\"ACC_train:\",losses[3])\n",
    "        print(\"F1_result:\",losses[4])\n",
    "        \n",
    "        ClassLoss_all.append(losses[0])\n",
    "        \n",
    "        ###### Save Model ###### \n",
    "        best_acc = 0.\n",
    "        best_ckpt = ''\n",
    "        if (i+1) % 100 == 0:  # 76\n",
    "            ckpt = '../EMNLP2023/Result/Model_HWU.pt'\n",
    "            torch.save(model, ckpt)\n",
    "    return None       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4647c7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main loop\n",
    "training(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66835c61",
   "metadata": {},
   "source": [
    "### Loss 下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b20f10be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = np.linspace(0, 2500, 2500)\n",
    "y1 = np.array([ClassLoss_all[i].item() for i in range(len(ClassLoss_all))])\n",
    "y2 = np.array([ACC_result[i].item() for i in range(len(ACC_result))])\n",
    "y3 = np.array([ACC_result[i].item() for i in range(len(F1_result))])\n",
    "\n",
    "plt.title('Loss Result Analysis in AORMmodel')\n",
    "# plt.plot(x, y1, color='red', label='Clustering_loss')\n",
    "# plt.plot(x, y2,  color='blue', label='ACC_result')\n",
    "plt.plot(x, y3,  color='green', label='F1_result')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7cbad",
   "metadata": {},
   "source": [
    "### 测试过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58ad8a7",
   "metadata": {},
   "source": [
    "#### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8f09982",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('../EMNLP2023/Result/Model_HWU.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92282391",
   "metadata": {},
   "source": [
    "#### 加载测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2b3c95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alarm remove', 'alarm remove', 'alarm remove', 'alarm remove', 'alarm remove']\n",
      "['disable the alarm which is set at nine thirty pm', 'delete scheduled alarm', 'turn off my first alarm', 'please cancel all alarms for tomorrow', 'please delete the wednesday evening alarm']\n",
      "All_y_id: 1674\n"
     ]
    }
   ],
   "source": [
    "#### Test_All\n",
    "All_path = '../EMNLP2023/Dataset/Processed/HWU/Test_all.txt'\n",
    "All_x = []\n",
    "All_y = []\n",
    "for line in open(All_path):\n",
    "    All_y.append(line.split('\\t')[1][:-1].replace('_',' '))\n",
    "    All_x.append(line.split('\\t')[0])\n",
    "print(All_y[:5])\n",
    "print(All_x[:5])\n",
    "\n",
    "pre_yd_list = label_yd_yi_dic.keys()\n",
    "pre_yd_yi_dic = label_yd_yi_dic\n",
    "\n",
    "All_y_id = []\n",
    "for i in range(len(All_y)):\n",
    "    if All_y[i] in pre_yd_list:\n",
    "        All_y_id.append(pre_yd_yi_dic[All_y[i]])\n",
    "    else:\n",
    "        All_y_id.append(cluster_nums_str)  \n",
    "print(\"All_y_id:\",len(All_y_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b059f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548\n",
      "32\n",
      "base_y_id: 548\n"
     ]
    }
   ],
   "source": [
    "base_x = []\n",
    "base_y = []\n",
    "for i in range(len(All_y)):\n",
    "    if All_y[i] != 'none of them':\n",
    "        base_x.append(All_x[i])\n",
    "        base_y.append(All_y[i])\n",
    "print(len(base_x))\n",
    "print(len(set(base_y)))\n",
    "\n",
    "base_y_id = []\n",
    "for i in range(len(base_y)):\n",
    "    if base_y[i] in label_yd_yi_dic.keys():\n",
    "        base_y_id.append(label_yd_yi_dic[base_y[i]])\n",
    "    else:\n",
    "        base_y_id.append(cluster_nums_str)  \n",
    "print(\"base_y_id:\",len(base_y_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf7b9f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am i available at five pm on sunday', 'do i have any openings on my schedule for tomorrow', 'do i need to pick up the kids from anything today', 'is the meeting scheduled for tomorrow', 'when the birthday will be']\n",
      "['none of them', 'none of them', 'none of them', 'none of them', 'none of them']\n",
      "Ct_y_id: 600\n"
     ]
    }
   ],
   "source": [
    "#### Ct_x\n",
    "Ct_path = '../EMNLP2023/Dataset/Processed/HWU/Test_ood_reduce.txt'\n",
    "Ct_x = []\n",
    "Ct_y = []\n",
    "for line in open(Ct_path):\n",
    "    Ct_y.append(line.split('\\t')[1][:-1].replace('_',' '))\n",
    "    Ct_x.append(line.split('\\t')[0])\n",
    "print(Ct_x[:5])\n",
    "print(Ct_y[:5])\n",
    "\n",
    "pre_yd_list = label_yd_yi_dic.keys()\n",
    "pre_yd_yi_dic = label_yd_yi_dic\n",
    "\n",
    "Ct_y_id = []\n",
    "for i in range(len(Ct_y)):\n",
    "    if Ct_y[i] in pre_yd_list:\n",
    "        Ct_y_id.append(pre_yd_yi_dic[Ct_y[i]])\n",
    "    else:\n",
    "        Ct_y_id.append(cluster_nums_str)  \n",
    "print(\"Ct_y_id:\",len(Ct_y_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1bcb18a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114.8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(548+600)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c188be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ct_x, Ct_y_id 60\n",
    "# base_x, base_y 191\n",
    "X = base_x + Ct_x\n",
    "Y = base_y_id + Ct_y_id\n",
    "B = 114\n",
    "test_dataset = AugmentPairSamples(X[:1140],Y[:1140])\n",
    "test_loader = util_data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0f20ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************0************\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'alarm set'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_803921/3550936817.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_none\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_task_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_contrastive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_none\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0my_true_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my_ture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0my_pre_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_803921/4240824454.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, inputs_text, inputs_none, labels, aggregate)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0my_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[20]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0my_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0my_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0my_onehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'alarm set'"
     ]
    }
   ],
   "source": [
    "y_true_list = []\n",
    "y_pre_list = []\n",
    "ACC_list = []\n",
    "F1_list = []\n",
    "for i in np.arange(B):  ## 25 epochs\n",
    "    model.training_flag = False        \n",
    "    print('************'+str(i)+'************')        \n",
    "    try:\n",
    "        batch = next(test_loader_iter)\n",
    "    except:\n",
    "        test_loader_iter = iter(test_loader)\n",
    "        batch = next(test_loader_iter)      \n",
    "\n",
    "    feats, features1, features_none, label = prepare_task_input(batch, is_contrastive=True)    \n",
    "    _, y_ture, y_pred, Acc, F1 = model.forward(feats, features1, features_none, label, aggregate=True)\n",
    "    y_true_list += y_ture\n",
    "    y_pre_list += y_pred\n",
    "    ACC_list += Acc\n",
    "    F1_list += F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00faa9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ALL\n",
    "print(\"ACC:\",accuracy_score(y_true_list,y_pre_list))\n",
    "print(\"F1:\",f1_score(y_true_list,y_pre_list, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c605bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##OOD\n",
    "y_pre_bi = []\n",
    "y_true_bi = []\n",
    "for i in range(len(y_pre_list)):\n",
    "    if y_pre_list[i] == cluster_nums_str:\n",
    "        y_pre_bi.append(1)\n",
    "    else:\n",
    "        y_pre_bi.append(0)\n",
    "##########\n",
    "for i in range(len(y_true_list)):\n",
    "    if y_true_list[i] == cluster_nums_str:\n",
    "        y_true_bi.append(1)\n",
    "    else:\n",
    "        y_true_bi.append(0)\n",
    "print(\"ACC:\",accuracy_score(y_true_bi,y_pre_bi))\n",
    "print(\"F1:\",f1_score(y_true_bi,y_pre_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f728f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc_all = [86.02, 86.61]\n",
    "F1_all = [86.67, 86.64]\n",
    "Acc_test = [73.33,72.67,73.17]\n",
    "F1_test = [84.62,84.17,84.50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e49c141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* 1 *********\n",
      "86.315\n",
      "0.41719300090006545\n",
      "********* 2 *********\n",
      "86.655\n",
      "0.02121320343559723\n",
      "********* 3 *********\n",
      "73.05666666666667\n",
      "0.3442867022313415\n",
      "********* 4 *********\n",
      "84.43\n",
      "0.23302360395462177\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import numpy as np\n",
    "\n",
    "print(\"********* 1 *********\")\n",
    "Acc_all_std = statistics.stdev(Acc_all)\n",
    "Acc_all_mean = np.mean(Acc_all)\n",
    "print(Acc_all_mean)\n",
    "print(Acc_all_std)\n",
    "\n",
    "print(\"********* 2 *********\")\n",
    "F1_all_std = statistics.stdev(F1_all)\n",
    "F1_all_mean = np.mean(F1_all)\n",
    "print(F1_all_mean)\n",
    "print(F1_all_std)\n",
    "\n",
    "print(\"********* 3 *********\")\n",
    "Acc_test_std = statistics.stdev(Acc_test)\n",
    "Acc_test_mean = np.mean(Acc_test)\n",
    "print(Acc_test_mean)\n",
    "print(Acc_test_std)\n",
    "\n",
    "print(\"********* 4 *********\")\n",
    "F1_test_std = statistics.stdev(F1_test)\n",
    "F1_test_mean = np.mean(F1_test)\n",
    "print(F1_test_mean)\n",
    "print(F1_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa9c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db1a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
