{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a802b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers.models.roberta.tokenization_roberta import RobertaTokenizer\n",
    "from transformers.optimization import AdamW\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3584ed81",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2d05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cbase_pretrain_path = '../Dataset/HWU/Pretrain.txt'\n",
    "Cbase_x = []\n",
    "Cbase_y = []\n",
    "for line in open(Cbase_pretrain_path):\n",
    "    Cbase_y.append(line.split('\\t')[1][:-1].replace('_',' '))\n",
    "    Cbase_x.append(line.split('\\t')[0])\n",
    "\n",
    "label_list = list(set(Cbase_y))\n",
    "label_yd_yi_dic = {}\n",
    "for i in range(len(label_list)):\n",
    "    label_yd_yi_dic[label_list[i]] = str(i)\n",
    "\n",
    "Cbase_y_label = []\n",
    "for i in range(len(Cbase_y)):\n",
    "    Cbase_y_label.append(label_yd_yi_dic[Cbase_y[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 32\n",
    "batch_size = 10\n",
    "N = 3000\n",
    "base_lr = 5e-6\n",
    "lr_scale = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682859ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### \n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as util_data\n",
    "\n",
    "class AugmentPairSamples(Dataset):\n",
    "    def __init__(self, train_x, train_y, train_y_des):\n",
    "        assert len(train_y) == len(train_x)\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.train_y_des = train_y_des\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'text': self.train_x[idx], 'label': self.train_y[idx], 'label_descrip': self.train_y_des[idx]}\n",
    "\n",
    "train_dataset = AugmentPairSamples(Cbase_x, Cbase_y_label, Cbase_y)\n",
    "train_loader = util_data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a3915",
   "metadata": {},
   "source": [
    "### Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e635e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0,1,2,3\"\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca38020",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_hidden_dim = 1024\n",
    "pretrain_model_dir = 'roberta-large'\n",
    "\n",
    "class RobertaClassificationHead(nn.Module):\n",
    "    def __init__(self, bert_hidden_dim, num_labels):\n",
    "        super(RobertaClassificationHead, self).__init__()\n",
    "        self.dense = nn.Linear(bert_hidden_dim, bert_hidden_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out_proj = nn.Linear(bert_hidden_dim, num_labels)\n",
    "    def forward(self, features):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "    \n",
    "class RobertaForSequenceClassification(nn.Module):\n",
    "    def __init__(self, tagset_size):\n",
    "        super(RobertaForSequenceClassification, self).__init__()\n",
    "        self.tagset_size = tagset_size\n",
    "\n",
    "        self.roberta_single= RobertaModel.from_pretrained(pretrain_model_dir)\n",
    "        self.single_hidden2tag = RobertaClassificationHead(bert_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, input_ids, input_mask):\n",
    "        outputs_single = self.roberta_single(input_ids, input_mask, None)\n",
    "        hidden_states_single = outputs_single[1]\n",
    "        score_single = self.single_hidden2tag(hidden_states_single) \n",
    "        return score_single\n",
    "\n",
    "pre_model = RobertaForSequenceClassification(3)\n",
    "pre_tokenizer = RobertaTokenizer.from_pretrained(pretrain_model_dir, do_lower_case=True)\n",
    "pre_model.load_state_dict(torch.load('../MNLI_pretrained.pt'), strict=False)\n",
    "pre_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a5095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PREModel(nn.Module):\n",
    "    def __init__(self, pre_tokenizer, pre_model, device=device, training_flag=True):\n",
    "        \n",
    "        super(PREModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.classifier_loss1 = nn.BCELoss()\n",
    "        self.classifier_loss2 = nn.CrossEntropyLoss()\n",
    "        self.training_flag = training_flag\n",
    "        self.optimizer = None\n",
    "        \n",
    "        ###### SentenceBert Model ###### \n",
    "        self.tokenizer = pre_tokenizer\n",
    "        self.sentbert = pre_model.roberta_single\n",
    "        \n",
    "        ####### Classifer Model ######  \n",
    "        self.classifer = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 1)).to(self.device)\n",
    "        \n",
    "    ###### SentenceBert Model ###### \n",
    "    def get_embeddings(self, features, pooling=\"mean\"):\n",
    "        bert_output =  self.sentbert.forward(**features)\n",
    "        attention_mask = features['attention_mask'].unsqueeze(-1).to(device)\n",
    "        all_output = bert_output[0]\n",
    "        mean_output =  torch.sum(all_output*attention_mask, dim=1) / torch.sum(attention_mask, dim=1)\n",
    "        return mean_output\n",
    "\n",
    "    def set_optmizer(self, opt):\n",
    "        self.optimizer = opt\n",
    "        \n",
    "    def forward(self, inputs, labels, aggregate=True):\n",
    "        for s_idx in range(len(labels)): #10\n",
    "            h0 = self.get_embeddings(inputs[s_idx], pooling=\"mean\").to(self.device) #[20,1024]\n",
    "            y_vector = torch.tensor([0.]*h0.shape[0]).to(self.device) #[20]\n",
    "            y_idx = int(labels[s_idx])\n",
    "            y_onehot = y_vector.clone()\n",
    "            y_onehot[y_idx] = 1.\n",
    "            \n",
    "            if s_idx == 0:\n",
    "                classifier_out = self.classifer(h0).squeeze().unsqueeze(0) #[1,20]\n",
    "                y_onehot_all = y_onehot.unsqueeze(0)\n",
    "            else:\n",
    "                classifier_out = torch.cat([classifier_out,self.classifer(h0).squeeze().unsqueeze(0)],dim=0) #[10,20]\n",
    "                y_onehot_all = torch.cat([y_onehot_all,y_onehot.unsqueeze(0)],dim=0)\n",
    "\n",
    "        classifier_output = classifier_out\n",
    "        classifier_sigmode_vector = F.sigmoid(classifier_output)\n",
    "        classifier_softmax_vector = F.softmax(classifier_output,dim=1)\n",
    "        cluster_result = [str(torch.argmax(classifier_softmax_vector[i]).item()) for i in range(classifier_softmax_vector.shape[0])]\n",
    "            \n",
    "        class_loss_all = torch.tensor(0.0).to(device)\n",
    "\n",
    "        class_loss_s = 0.\n",
    "        class_loss_s1 = self.classifier_loss1(classifier_sigmode_vector,y_onehot_all)\n",
    "        class_loss_s2 = self.classifier_loss2(classifier_softmax_vector,y_onehot_all)\n",
    "        class_loss_all += class_loss_s1 + class_loss_s2\n",
    "        class_loss = torch.div(class_loss_all,h0.shape[0])\n",
    "        \n",
    "        if self.training_flag:\n",
    "            loss = class_loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return class_loss.detach(), class_loss_s1.detach(), class_loss_s2.detach(), labels, cluster_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PREModel(pre_tokenizer=pre_tokenizer, pre_model=pre_model, device=device, training_flag=True).to(device)\n",
    "base_lr = 5e-6\n",
    "lr_scale = 100\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':model.sentbert.parameters()}, \n",
    "    {'params':model.classifer.parameters(), 'lr':base_lr * lr_scale}], base_lr)\n",
    "\n",
    "model.set_optmizer(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f49f5",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932acf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_task_input(batch, is_contrastive=False):\n",
    "    if is_contrastive:\n",
    "        text, label = batch['text'], batch['label']\n",
    "        all_label_descrip = list(label_yd_yi_dic.keys())\n",
    "        feat = {'input_ids': torch.tensor(0),'attention_mask':torch.tensor(0)}\n",
    "        features1 = model.tokenizer.batch_encode_plus(text, max_length=max_length, return_tensors='pt', padding='longest', \n",
    "                                                         truncation=True)\n",
    "        features2 = model.tokenizer.batch_encode_plus(all_label_descrip, max_length=max_length, return_tensors='pt', padding='longest', \n",
    "                                                         truncation=True)\n",
    "        feat1_inputids = features1[\"input_ids\"]\n",
    "        feat1_attmask = features1[\"attention_mask\"]\n",
    "        feat2_inputids = features2[\"input_ids\"]\n",
    "        feat2_attmask = features2[\"attention_mask\"]\n",
    "        max_len = feat1_inputids.shape[1] + feat2_inputids.shape[1] \n",
    "        \n",
    "        non_zero1 = torch.count_nonzero(feat1_attmask, dim=1).reshape(-1, 1)\n",
    "        non_zero2 = torch.count_nonzero(feat2_attmask, dim=1).reshape(-1, 1)\n",
    "        \n",
    "        feats = []\n",
    "        for i in range(non_zero1.shape[0]):\n",
    "            index1 = non_zero1[i][0]\n",
    "            feat_s = {'input_ids': torch.tensor(0),'attention_mask':torch.tensor(0)}\n",
    "            for j in range(non_zero2.shape[0]):\n",
    "                index2 = non_zero2[j][0]\n",
    "                feat_inputid = torch.cat([feat2_inputids[j][:index2],feat1_inputids[i][1:index1]])\n",
    "                feat_attmask = torch.tensor([1]*(index1+index2)+[0]*(max_len-index1-index2)).unsqueeze(0)\n",
    "                feat_inputid = torch.cat([feat_inputid,torch.tensor([1]*(max_len-index1-index2+1))]).unsqueeze(0)\n",
    "                if j == 0:\n",
    "                    feat_inputid_s = feat_inputid\n",
    "                    feat_attmask_s = feat_attmask\n",
    "                elif j != 0:\n",
    "                    feat_inputid_s = torch.cat([feat_inputid_s, feat_inputid],dim=0)\n",
    "                    feat_attmask_s = torch.cat([feat_attmask_s, feat_attmask],dim=0)\n",
    "            feat_s['input_ids'] = feat_inputid_s.to(device)\n",
    "            feat_s['attention_mask'] = feat_attmask_s.to(device)\n",
    "            feats.append(feat_s)  \n",
    "        \n",
    "        return feats, label\n",
    "    \n",
    "InsCL_all = []\n",
    "SupCL_all = []\n",
    "def training(train_loader):\n",
    "    pre_acc = 0.\n",
    "    epoch = 0\n",
    "    ACC_result = []\n",
    "    for i in np.arange(N):  \n",
    "        model.train()\n",
    "        model.training_flag = True        \n",
    "        print('************'+str(i)+'************')        \n",
    "        try:\n",
    "            batch = next(train_loader_iter)\n",
    "        except:\n",
    "            train_loader_iter = iter(train_loader)\n",
    "            batch = next(train_loader_iter)      \n",
    "        \n",
    "        feats, labels = prepare_task_input(batch, is_contrastive=True)    \n",
    "        losses = model.forward(feats, labels, aggregate=True)\n",
    "        ACC_result.append(losses[0])\n",
    "\n",
    "        ###### Save Model ###### \n",
    "        best_acc = 0.\n",
    "        best_ckpt = ''\n",
    "        if (i+1) % 100 == 0:  # 76\n",
    "            ckpt = '../Result/PreModel.pt'\n",
    "            torch.save(model, ckpt)\n",
    "    return None       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8769cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training(train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
