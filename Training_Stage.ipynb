{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070cb8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers.models.roberta.tokenization_roberta import RobertaTokenizer\n",
    "from transformers.optimization import AdamW\n",
    "from transformers.models.roberta.modeling_roberta import RobertaModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b7bc69",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205baafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cbase_pretrain_path = '../Dataset/HWU/Pretrain.txt'\n",
    "Cbase_x = []\n",
    "Cbase_y = []\n",
    "for line in open(Cbase_pretrain_path):\n",
    "    if len(line.split('\\t')) != 2:\n",
    "        continue\n",
    "    else:\n",
    "        Cbase_y.append(line.split('\\t')[1][:-1].replace('_',' '))\n",
    "        Cbase_x.append(line.split('\\t')[0])\n",
    "\n",
    "label_list = list(set(Cbase_y))\n",
    "label_yd_yi_dic = {}\n",
    "for i in range(len(label_list)):\n",
    "    label_yd_yi_dic[label_list[i]] = str(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8f7cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 32\n",
    "n_tokens = 5\n",
    "hidden_size = 1024\n",
    "batch_size = 10\n",
    "cluster_nums = len(list(set(Cbase_y)))\n",
    "cluster_nums_str = str(cluster_nums)\n",
    "N = 2500\n",
    "base_lr = 5e-6\n",
    "lr_scale = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e613357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cn_path = '../Dataset/HWU/Aux1PMulti.txt'\n",
    "Cn = []\n",
    "for line in open(Cn_path):\n",
    "    Cn.append(line)\n",
    "random.shuffle(Cn)\n",
    "\n",
    "Cn_x = []\n",
    "Cn_y = []\n",
    "for line in Cn:\n",
    "    Cn_y.append(line.split('\\t')[1][:-1].replace('_',' '))\n",
    "    Cn_x.append(line.split('\\t')[0])\n",
    "\n",
    "pre_yd_list = label_yd_yi_dic.keys()\n",
    "pre_yd_yi_dic = label_yd_yi_dic\n",
    "\n",
    "Cn_y_id = []\n",
    "for i in range(len(Cn_y)):\n",
    "    if Cn_y[i] in pre_yd_list:\n",
    "        Cn_y_id.append(pre_yd_yi_dic[Cn_y[i]])\n",
    "    else:\n",
    "        Cn_y_id.append(cluster_nums_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### \n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data as util_data\n",
    "\n",
    "class AugmentPairSamples(Dataset):\n",
    "    def __init__(self, train_x, train_y):\n",
    "        assert len(train_y) == len(train_x)\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'text': self.train_x[idx], 'label': self.train_y[idx]}\n",
    "\n",
    "train_dataset = AugmentPairSamples(Cn_x, Cn_y_id)\n",
    "train_loader = util_data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54a64f5",
   "metadata": {},
   "source": [
    "### 加载 Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806cc84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0,1,2,3\"\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b87d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PREModel(nn.Module):\n",
    "    def __init__(self, pre_tokenizer, pre_model, device=device, training_flag=True):\n",
    "        \n",
    "        super(PREModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.classifier_loss1 = nn.BCELoss()\n",
    "        self.classifier_loss2 = nn.CrossEntropyLoss()\n",
    "        self.training_flag = training_flag\n",
    "        self.optimizer = None\n",
    "        \n",
    "        ###### SentenceBert Model ###### \n",
    "        self.tokenizer = pre_tokenizer\n",
    "        self.sentbert = pre_model.roberta_single\n",
    "        \n",
    "        ####### Classifer Model ######  \n",
    "        self.classifer = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 1)).to(self.device)\n",
    "        \n",
    "    ###### SentenceBert Model ###### \n",
    "    def get_embeddings(self, features, pooling=\"mean\"):\n",
    "        bert_output =  self.sentbert.forward(**features)\n",
    "        attention_mask = features['attention_mask'].unsqueeze(-1).to(device)\n",
    "        all_output = bert_output[0]\n",
    "        mean_output =  torch.sum(all_output*attention_mask, dim=1) / torch.sum(attention_mask, dim=1)\n",
    "        return mean_output\n",
    "\n",
    "    def set_optmizer(self, opt):\n",
    "        self.optimizer = opt\n",
    "        \n",
    "    def forward(self, inputs, labels, aggregate=True):\n",
    "        for s_idx in range(len(labels)): \n",
    "            h0 = self.get_embeddings(inputs[s_idx], pooling=\"mean\").to(self.device) \n",
    "            y_vector = torch.tensor([0.]*h0.shape[0]).to(self.device) \n",
    "            y_idx = int(labels[s_idx])\n",
    "            y_onehot = y_vector.clone()\n",
    "            y_onehot[y_idx] = 1.\n",
    "            \n",
    "            if s_idx == 0:\n",
    "                classifier_out = self.classifer(h0).squeeze().unsqueeze(0) \n",
    "                y_onehot_all = y_onehot.unsqueeze(0)\n",
    "            else:\n",
    "                classifier_out = torch.cat([classifier_out,self.classifer(h0).squeeze().unsqueeze(0)],dim=0) \n",
    "                y_onehot_all = torch.cat([y_onehot_all,y_onehot.unsqueeze(0)],dim=0)\n",
    "\n",
    "        classifier_output = classifier_out\n",
    "        classifier_sigmode_vector = F.sigmoid(classifier_output)\n",
    "        classifier_softmax_vector = F.softmax(classifier_output,dim=1)\n",
    "        cluster_result = [str(torch.argmax(classifier_softmax_vector[i]).item()) for i in range(classifier_softmax_vector.shape[0])]\n",
    "            \n",
    "        class_loss_all = torch.tensor(0.0).to(device)\n",
    "        class_loss_s = 0.\n",
    "        class_loss_s1 = self.classifier_loss1(classifier_sigmode_vector,y_onehot_all)\n",
    "        class_loss_s2 = self.classifier_loss2(classifier_softmax_vector,y_onehot_all)\n",
    "        class_loss_all += class_loss_s1 + class_loss_s2\n",
    "        class_loss = torch.div(class_loss_all,h0.shape[0])\n",
    "        \n",
    "        if self.training_flag:\n",
    "            loss = class_loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return class_loss.detach(), class_loss_s1.detach(), class_loss_s2.detach(), labels, cluster_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c7c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = '../Result/PreModel.pt'\n",
    "pre_model = torch.load(ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0dcb89",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a9bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRAINModel(nn.Module):\n",
    "    def __init__(self, pre_tokenizer, pre_encoder, pre_classifer, device=device, training_flag=True):\n",
    "        \n",
    "        super(TRAINModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.classifier_loss1 = nn.BCELoss()\n",
    "        self.classifier_loss2 = nn.CrossEntropyLoss()\n",
    "        self.training_flag = training_flag\n",
    "        self.optimizer = None\n",
    "        \n",
    "        ###### SentenceBert Model ###### \n",
    "        self.tokenizer = pre_tokenizer\n",
    "        self.sentbert = pre_encoder\n",
    "        \n",
    "        ####### Classifer Model ######  \n",
    "        self.classifer = pre_classifer\n",
    "        \n",
    "    ###### SentenceBert Model ###### \n",
    "    def get_embeddings(self, features, pooling=\"mean\"):\n",
    "        bert_output =  self.sentbert.forward(**features)\n",
    "        attention_mask = features['attention_mask'].unsqueeze(-1).to(device)\n",
    "        all_output = bert_output[0]\n",
    "        mean_output =  torch.sum(all_output*attention_mask, dim=1) / torch.sum(attention_mask, dim=1)\n",
    "        return mean_output\n",
    "    \n",
    "    def get_none_embeddings(self, input_t, input_none, pooling=\"mean\"):\n",
    "        x_embed = self.sentbert.embeddings.word_embeddings(input_t['input_ids']) \n",
    "        x_embed_cls = self.sentbert.embeddings.word_embeddings(input_t['input_ids'])[:,0,:].unsqueeze(1) \n",
    "        none_embeddings_inital = self.sentbert.forward(**input_none).last_hidden_state \n",
    "        soft_text_prompt = nn.Embedding(n_tokens, hidden_size)\n",
    "        soft_text_prompt.weight = nn.parameter.Parameter(none_embeddings_inital) \n",
    "        return x_embed_cls, x_embed, soft_text_prompt.weight\n",
    "\n",
    "    def set_optmizer(self, opt):\n",
    "        self.optimizer = opt\n",
    "        \n",
    "    def forward(self, inputs, inputs_text, inputs_none, labels, aggregate=True):\n",
    "        for s_idx in range(len(labels)): #10\n",
    "            h0 = self.get_embeddings(inputs[s_idx], pooling=\"mean\").to(self.device) \n",
    "            \n",
    "            y_vector = torch.tensor([0.]*(h0.shape[0]+1)).to(self.device)\n",
    "            y_idx = int(labels[s_idx])\n",
    "            y_onehot = y_vector.clone()\n",
    "            y_onehot[y_idx] = 1.\n",
    "            \n",
    "            if s_idx == 0:\n",
    "                classifier_out = self.classifer(h0).squeeze().unsqueeze(0) \n",
    "                y_onehot_all = y_onehot.unsqueeze(0)\n",
    "            else:\n",
    "                classifier_out = torch.cat([classifier_out,self.classifer(h0).squeeze().unsqueeze(0)],dim=0)\n",
    "                y_onehot_all = torch.cat([y_onehot_all,y_onehot.unsqueeze(0)],dim=0)\n",
    "                \n",
    "        x_embed_cls, x_embed, soft_text_prompt_weight = self.get_none_embeddings(inputs_text.to(self.device),inputs_none.to(self.device))\n",
    "        x_attmask = inputs_text['attention_mask'] \n",
    "        x_ones = torch.ones([batch_size,n_tokens]).to(device) \n",
    "        none_attmask = torch.cat([x_ones,x_attmask],dim=1).to(device)\n",
    "\n",
    "        none_embedding = soft_text_prompt_weight.repeat(batch_size,1,1)\n",
    "        none_encoderinput = torch.cat([none_embedding,x_embed],dim=1)\n",
    "\n",
    "        text_input = {\n",
    "            'inputs_embeds': none_encoderinput,\n",
    "            'attention_mask': none_attmask.to(device)\n",
    "        }\n",
    "        \n",
    "        h0_none = self.sentbert.forward(**text_input).pooler_output\n",
    "        classifier_out_none = self.classifer(h0_none)\n",
    "        classifier_output = torch.cat([classifier_out,classifier_out_none],dim=1)\n",
    "        classifier_sigmode_vector = F.sigmoid(classifier_output)\n",
    "        classifier_softmax_vector = F.softmax(classifier_output,dim=1)\n",
    "        cluster_result = [str(torch.argmax(classifier_softmax_vector[i]).item()) for i in range(classifier_softmax_vector.shape[0])]\n",
    "        class_loss_all = torch.tensor(0.0).to(device)\n",
    "        \n",
    "        class_loss_s = 0.\n",
    "        class_loss_s1 = self.classifier_loss1(classifier_sigmode_vector,y_onehot_all)\n",
    "        class_loss_s2 = self.classifier_loss2(classifier_softmax_vector,y_onehot_all)\n",
    "        class_loss_all += class_loss_s1 + class_loss_s2\n",
    "        class_loss = torch.div(class_loss_all,h0.shape[0])\n",
    "        \n",
    "        ACC_train = accuracy_score(labels,cluster_result)\n",
    "        F1_train = f1_score(labels, cluster_result, average='macro')\n",
    "          \n",
    "        if self.training_flag:\n",
    "            loss = class_loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return class_loss.detach(), labels, cluster_result, ACC_train, F1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b8a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TRAINModel(pre_tokenizer=pre_model.tokenizer, pre_encoder=pre_model.sentbert, pre_classifer=pre_model.classifer, device=device, training_flag=True).to(device)\n",
    "base_lr = 5e-6\n",
    "lr_scale = 100\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':model.sentbert.parameters()}, \n",
    "    {'params':model.classifer.parameters(), 'lr':base_lr * lr_scale}], base_lr)\n",
    "\n",
    "model.set_optmizer(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3cf76e",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_task_input(batch, is_contrastive=False):\n",
    "    if is_contrastive:\n",
    "        text, label = batch['text'], batch['label']\n",
    "        all_label_descrip = list(label_yd_yi_dic.keys())\n",
    "        feat = {'input_ids': torch.tensor(0),'attention_mask':torch.tensor(0)}\n",
    "        features1 = model.tokenizer.batch_encode_plus(text, max_length=max_length, return_tensors='pt', padding='longest', \n",
    "                                                         truncation=True)\n",
    "        features2 = model.tokenizer.batch_encode_plus(all_label_descrip, max_length=max_length, return_tensors='pt', padding='longest', \n",
    "                                                         truncation=True)\n",
    "        features_none = model.tokenizer.batch_encode_plus(['none of them'], max_length=max_length, return_tensors='pt', padding='longest', \n",
    "                                                         truncation=True)\n",
    "        \n",
    "        feat1_inputids = features1[\"input_ids\"]\n",
    "        feat1_attmask = features1[\"attention_mask\"]\n",
    "        feat2_inputids = features2[\"input_ids\"]\n",
    "        feat2_attmask = features2[\"attention_mask\"]\n",
    "        max_len = feat1_inputids.shape[1] + feat2_inputids.shape[1] \n",
    "        \n",
    "        # count\n",
    "        non_zero1 = torch.count_nonzero(feat1_attmask, dim=1).reshape(-1, 1)\n",
    "        non_zero2 = torch.count_nonzero(feat2_attmask, dim=1).reshape(-1, 1)\n",
    "        \n",
    "        feats = []\n",
    "        for i in range(non_zero1.shape[0]):\n",
    "            index1 = non_zero1[i][0]\n",
    "            feat_s = {'input_ids': torch.tensor(0),'attention_mask':torch.tensor(0)}\n",
    "            for j in range(non_zero2.shape[0]):\n",
    "                index2 = non_zero2[j][0]\n",
    "                feat_inputid = torch.cat([feat2_inputids[j][:index2],feat1_inputids[i][1:index1]])\n",
    "                feat_attmask = torch.tensor([1]*(index1+index2)+[0]*(max_len-index1-index2)).unsqueeze(0)\n",
    "                feat_inputid = torch.cat([feat_inputid,torch.tensor([1]*(max_len-index1-index2+1))]).unsqueeze(0)\n",
    "                if j == 0:\n",
    "                    feat_inputid_s = feat_inputid\n",
    "                    feat_attmask_s = feat_attmask\n",
    "                elif j != 0:\n",
    "                    feat_inputid_s = torch.cat([feat_inputid_s, feat_inputid],dim=0)\n",
    "                    feat_attmask_s = torch.cat([feat_attmask_s, feat_attmask],dim=0)\n",
    "            feat_s['input_ids'] = feat_inputid_s.to(device)\n",
    "            feat_s['attention_mask'] = feat_attmask_s.to(device)\n",
    "            feats.append(feat_s)  \n",
    "        \n",
    "        return feats, features1, features_none, label\n",
    "    \n",
    "ClassLoss_all = []\n",
    "ACC_result = []\n",
    "F1_result = []\n",
    "def training(train_loader):\n",
    "    pre_acc = 0.\n",
    "    epoch = 0\n",
    "    for i in np.arange(N):\n",
    "        model.train()\n",
    "        model.training_flag = True        \n",
    "        print('************'+str(i)+'************')        \n",
    "        try:\n",
    "            batch = next(train_loader_iter)\n",
    "        except:\n",
    "            train_loader_iter = iter(train_loader)\n",
    "            batch = next(train_loader_iter)      \n",
    "        \n",
    "        feats, features1, features_none, label = prepare_task_input(batch, is_contrastive=True)    \n",
    "        losses = model.forward(feats, features1, features_none, label, aggregate=True)\n",
    "        \n",
    "        ACC_result.append(losses[3])\n",
    "        F1_result.append(losses[4])\n",
    "        \n",
    "        ClassLoss_all.append(losses[0])\n",
    "        \n",
    "        ###### Save Model ###### \n",
    "        best_acc = 0.\n",
    "        best_ckpt = ''\n",
    "        if (i+1) % 100 == 0:  # 76\n",
    "            ckpt = '../Result/Model.pt'\n",
    "            torch.save(model, ckpt)\n",
    "    return None       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1c80a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68690d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
